{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d207600",
   "metadata": {},
   "source": [
    "# KOCH PAIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb940958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num beats: 127\n",
      "ECG batch shape: torch.Size([8, 32, 2000])\n",
      "MCG batch shape: torch.Size([8, 100, 2000])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class KochPairedBeatsDataset(Dataset):\n",
    "    def __init__(self, npz_path=\"koch_pairs.npz\", augment=False):\n",
    "        \"\"\"\n",
    "        npz_path: path to the koch_pairs.npz file\n",
    "        augment: if True, apply very light noise / jitter augmentations\n",
    "        \"\"\"\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        self.ecg_beats = data[\"ecg_beats\"]  # (N, C_ecg, T)\n",
    "        self.mcg_beats = data[\"mcg_beats\"]  # (N, C_mcg, T)\n",
    "        self.fs = int(data[\"fs\"][0])\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ecg_beats.shape[0]\n",
    "\n",
    "    def _augment(self, x):\n",
    "        \"\"\"\n",
    "        x: numpy array (C, T)\n",
    "        very light augmentations: small Gaussian noise and tiny time shift\n",
    "        \"\"\"\n",
    "        # small noise\n",
    "        x = x + 0.01 * np.random.randn(*x.shape).astype(np.float32)\n",
    "\n",
    "        # tiny circular time shift up to ±20 samples\n",
    "        max_shift = 20\n",
    "        shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "        if shift != 0:\n",
    "            x = np.roll(x, shift, axis=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ecg = self.ecg_beats[idx]  # (C_ecg, T)\n",
    "        mcg = self.mcg_beats[idx]  # (C_mcg, T)\n",
    "\n",
    "        if self.augment:\n",
    "            ecg = self._augment(ecg)\n",
    "            mcg = self._augment(mcg)\n",
    "\n",
    "        # convert to torch tensors\n",
    "        ecg = torch.from_numpy(ecg)  # float32, shape (C_ecg, T)\n",
    "        mcg = torch.from_numpy(mcg)  # float32, shape (C_mcg, T)\n",
    "\n",
    "        return ecg, mcg\n",
    "\n",
    "\n",
    "#from koch_dataset import KochPairedBeatsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = KochPairedBeatsDataset(\"koch_pairs.npz\", augment=False)\n",
    "print(\"Num beats:\", len(ds))\n",
    "\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True)\n",
    "ecg, mcg = next(iter(loader))\n",
    "print(\"ECG batch shape:\", ecg.shape)  # expect: torch.Size([8, 32, 2000])\n",
    "print(\"MCG batch shape:\", mcg.shape)  # expect: torch.Size([8, 100, 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec958b0",
   "metadata": {},
   "source": [
    "# PTB Beats Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a1a8383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12, 2000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PTBBeatsDataset(Dataset):\n",
    "    def __init__(self, npz_path=\"ptb_beats.npz\", augment=True):\n",
    "        data = np.load(npz_path, allow_pickle=True)\n",
    "        self.beats = data[\"beats\"]        # (N, 12, 2000)\n",
    "        self.fs = int(data[\"fs\"][0])\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.beats.shape[0]\n",
    "\n",
    "    def _augment(self, x):\n",
    "        # x: (12, T)\n",
    "        x = x + 0.01 * np.random.randn(*x.shape).astype(np.float32)  # noise\n",
    "        max_shift = 20\n",
    "        shift = np.random.randint(-max_shift, max_shift + 1)\n",
    "        if shift != 0:\n",
    "            x = np.roll(x, shift, axis=1)\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        beat = self.beats[idx]\n",
    "        if self.augment:\n",
    "            beat = self._augment(beat)\n",
    "        return torch.from_numpy(beat)\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds_ptb = PTBBeatsDataset(\"ptb_beats.npz\", augment=True)\n",
    "loader_ptb = DataLoader(ds_ptb, batch_size=32, shuffle=True)\n",
    "\n",
    "batch = next(iter(loader_ptb))\n",
    "print(batch.shape)  # torch.Size([32, 12, 2000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119102e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(in_ch, in_ch, kernel_size=kernel_size,\n",
    "                                   stride=stride, padding=padding, groups=in_ch)\n",
    "        self.pointwise = nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiScaleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale depthwise-separable residual block.\n",
    "    Input: (B, C, T) -> Output: (B, C, T)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernels=(5, 9, 17)):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList()\n",
    "        for k in kernels:\n",
    "            pad = k // 2\n",
    "            self.branches.append(\n",
    "                DepthwiseSeparableConv1d(channels, channels, kernel_size=k, padding=pad)\n",
    "            )\n",
    "        self.bn = nn.BatchNorm1d(channels)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T)\n",
    "        outs = []\n",
    "        for conv in self.branches:\n",
    "            outs.append(conv(x))\n",
    "        out = sum(outs) / len(outs)  # average branches\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        return x + out  # residual\n",
    "\n",
    "\n",
    "class SMEEBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared Multi-Scale Efficient Encoder backbone.\n",
    "    Input: (B, C_bottleneck, T) -> Output: (B, feat_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, bottleneck_channels=32, n_blocks=3, feat_dim=256):\n",
    "        super().__init__()\n",
    "        blocks = []\n",
    "        for _ in range(n_blocks):\n",
    "            blocks.append(MultiScaleBlock(bottleneck_channels, kernels=(5, 9, 17)))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(bottleneck_channels, feat_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C_bottleneck, T)\n",
    "        x = self.blocks(x)                 # (B, C_bottleneck, T)\n",
    "        x = self.global_pool(x).squeeze(-1)  # (B, C_bottleneck)\n",
    "        x = self.fc(x)                     # (B, feat_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ECGEncoderSMEE(nn.Module):\n",
    "    \"\"\"\n",
    "    ECG encoder using SMEE backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=32, bottleneck_channels=32, feat_dim=256):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(in_channels, bottleneck_channels, kernel_size=1)\n",
    "        self.backbone = SMEEBackbone(bottleneck_channels=bottleneck_channels,\n",
    "                                     n_blocks=3, feat_dim=feat_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MCGEncoderSMEE(nn.Module):\n",
    "    \"\"\"\n",
    "    MCG encoder using SMEE backbone. Can share backbone weights with ECG encoder if desired.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 100,\n",
    "        bottleneck_channels: int = 32,\n",
    "        feat_dim: int = 256,\n",
    "        shared_backbone: Optional[SMEEBackbone] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Conv1d(in_channels, bottleneck_channels, kernel_size=1)\n",
    "        if shared_backbone is None:\n",
    "            self.backbone = SMEEBackbone(\n",
    "                bottleneck_channels=bottleneck_channels,\n",
    "                n_blocks=3,\n",
    "                feat_dim=feat_dim,\n",
    "            )\n",
    "        else:\n",
    "            self.backbone = shared_backbone  # weight sharing\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "550e86a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG SMEE params: 22464\n",
      "MCG SMEE params: 24640\n",
      "Shapes: torch.Size([4, 256]) torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "#from models import ECGEncoderSMEE, MCGEncoderSMEE  # or whatever file name you used\n",
    "import torch\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "ecg_smee = ECGEncoderSMEE(in_channels=32)\n",
    "mcg_smee = MCGEncoderSMEE(in_channels=100)\n",
    "\n",
    "print(\"ECG SMEE params:\", count_params(ecg_smee))\n",
    "print(\"MCG SMEE params:\", count_params(mcg_smee))\n",
    "\n",
    "# quick forward test\n",
    "x_ecg = torch.randn(4, 32, 2000)\n",
    "x_mcg = torch.randn(4, 100, 2000)\n",
    "h_e = ecg_smee(x_ecg)\n",
    "h_m = mcg_smee(x_mcg)\n",
    "print(\"Shapes:\", h_e.shape, h_m.shape)  # should be: torch.Size([4, 256]) torch.Size([4, 256])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41f653",
   "metadata": {},
   "source": [
    "# Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bff868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer MLP projection head.\n",
    "    Input: (B, feat_dim)\n",
    "    Output: (B, proj_dim) normalized\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, in_dim)\n",
    "        self.fc2 = nn.Linear(in_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def cross_modal_info_nce(z_e, z_m, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Cross-modal InfoNCE loss between ECG (z_e) and MCG (z_m).\n",
    "    z_e: (B, D) ECG embeddings (L2-normalized)\n",
    "    z_m: (B, D) MCG embeddings (L2-normalized)\n",
    "\n",
    "    We compute similarity matrix S = z_e @ z_m^T / T\n",
    "    and use symmetric loss: ECG→MCG and MCG→ECG.\n",
    "    \"\"\"\n",
    "    assert z_e.shape == z_m.shape\n",
    "    B, D = z_e.shape\n",
    "\n",
    "    # cosine similarity (since both are normalized, dot = cos)\n",
    "    logits = z_e @ z_m.T / temperature  # (B, B)\n",
    "\n",
    "    targets = torch.arange(B, device=z_e.device)\n",
    "\n",
    "    # ECG→MCG\n",
    "    loss_e2m = F.cross_entropy(logits, targets)\n",
    "\n",
    "    # MCG→ECG (transpose)\n",
    "    loss_m2e = F.cross_entropy(logits.T, targets)\n",
    "\n",
    "    loss = 0.5 * (loss_e2m + loss_m2e)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabde47",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e66b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def simclr_nt_xent_loss(z1, z2, temperature=0.1):\n",
    "    \"\"\"\n",
    "    z1, z2: (B, D) normalized embeddings from two views of the same batch.\n",
    "    Returns scalar loss.\n",
    "    \"\"\"\n",
    "    assert z1.shape == z2.shape\n",
    "    batch_size = z1.shape[0]\n",
    "\n",
    "    z = torch.cat([z1, z2], dim=0)  # (2B, D)\n",
    "    sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=-1)  # (2B, 2B)\n",
    "\n",
    "    # Mask to remove self-similarity\n",
    "    self_mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z.device)\n",
    "    sim = sim / temperature\n",
    "\n",
    "    # For each anchor i in 0..2B-1, define positives and negatives\n",
    "    # Positives: (i, i+B) or (i, i-B) depending on which half\n",
    "    labels = torch.arange(2 * batch_size, device=z.device)\n",
    "    labels = (labels + batch_size) % (2 * batch_size)  # positive index for each anchor\n",
    "\n",
    "    # For cross-entropy, we need logits (2B, 2B-1) and labels\n",
    "    sim = sim.masked_fill(self_mask, -1e9)\n",
    "\n",
    "    loss = F.cross_entropy(sim, labels)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23991e",
   "metadata": {},
   "source": [
    "# train_koch_crossmodal_smee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "473fada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 001/10 - SMEE cross-modal loss: 2.0847\n",
      "Epoch 002/10 - SMEE cross-modal loss: 2.0599\n",
      "Epoch 003/10 - SMEE cross-modal loss: 2.0683\n",
      "Epoch 004/10 - SMEE cross-modal loss: 1.9906\n",
      "Epoch 005/10 - SMEE cross-modal loss: 1.8966\n",
      "Epoch 006/10 - SMEE cross-modal loss: 1.9340\n",
      "Epoch 007/10 - SMEE cross-modal loss: 1.7252\n",
      "Epoch 008/10 - SMEE cross-modal loss: 1.5543\n",
      "Epoch 009/10 - SMEE cross-modal loss: 1.3880\n",
      "Epoch 010/10 - SMEE cross-modal loss: 1.2995\n",
      "Saved SMEE-based ECG + MCG encoders and projection heads (Koch dataset)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from koch_dataset import KochPairedBeatsDataset\n",
    "#from models import (ECGEncoderSMEE,MCGEncoderSMEE,ProjectionHead, cross_modal_info_nce,   )\n",
    "\n",
    "\n",
    "def train_koch_crossmodal_smee(\n",
    "    npz_path=\"koch_pairs.npz\",\n",
    "    batch_size=8,\n",
    "    lr=1e-3,\n",
    "    epochs=50,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    # 1) Dataset & DataLoader\n",
    "    dataset = KochPairedBeatsDataset(npz_path=npz_path, augment=True)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # 2) New encoders: SMEE-based ECG (32 ch) & MCG (100 ch)\n",
    "    #    (no shared backbone yet, we keep it simple first)\n",
    "    ecg_encoder = ECGEncoderSMEE(in_channels=32, bottleneck_channels=32, feat_dim=256)\n",
    "    mcg_encoder = MCGEncoderSMEE(in_channels=100, bottleneck_channels=32, feat_dim=256)\n",
    "\n",
    "    ecg_proj = ProjectionHead(in_dim=256, proj_dim=128)\n",
    "    mcg_proj = ProjectionHead(in_dim=256, proj_dim=128)\n",
    "\n",
    "    ecg_encoder.to(device)\n",
    "    mcg_encoder.to(device)\n",
    "    ecg_proj.to(device)\n",
    "    mcg_proj.to(device)\n",
    "\n",
    "    # 3) Optimizer\n",
    "    params = (\n",
    "        list(ecg_encoder.parameters())\n",
    "        + list(mcg_encoder.parameters())\n",
    "        + list(ecg_proj.parameters())\n",
    "        + list(mcg_proj.parameters())\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # 4) Training loop\n",
    "    ecg_encoder.train()\n",
    "    mcg_encoder.train()\n",
    "    ecg_proj.train()\n",
    "    mcg_proj.train()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, (ecg, mcg) in enumerate(loader):\n",
    "            ecg = ecg.to(device)  # (B, 32, 2000)\n",
    "            mcg = mcg.to(device)  # (B, 100, 2000)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Encode\n",
    "            h_e = ecg_encoder(ecg)   # (B, 256)\n",
    "            h_m = mcg_encoder(mcg)   # (B, 256)\n",
    "\n",
    "            # Project\n",
    "            z_e = ecg_proj(h_e)      # (B, 128)\n",
    "            z_m = mcg_proj(h_m)      # (B, 128)\n",
    "\n",
    "            # Cross-modal InfoNCE\n",
    "            loss = cross_modal_info_nce(z_e, z_m, temperature=0.1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / (step + 1)\n",
    "        print(f\"Epoch {epoch:03d}/{epochs} - SMEE cross-modal loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 5) Save new model weights\n",
    "    torch.save(ecg_encoder.state_dict(), \"ecg_encoder_koch_smee.pth\")\n",
    "    torch.save(mcg_encoder.state_dict(), \"mcg_encoder_koch_smee.pth\")\n",
    "    torch.save(ecg_proj.state_dict(), \"ecg_proj_koch_smee.pth\")\n",
    "    torch.save(mcg_proj.state_dict(), \"mcg_proj_koch_smee.pth\")\n",
    "    print(\"Saved SMEE-based ECG + MCG encoders and projection heads (Koch dataset)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    train_koch_crossmodal_smee(\n",
    "        npz_path=\"koch_pairs.npz\",\n",
    "        batch_size=8,\n",
    "        lr=1e-3,\n",
    "        epochs=10,   # start with 30–50 to check things; you can increase later\n",
    "        device=device,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310e2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8971313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_on",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
